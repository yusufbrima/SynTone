{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from celluloid import Camera\n",
    "from IPython.display import Audio\n",
    "import librosa\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from Models.vae import VAE,VAEDeep,VAEDeeper\n",
    "from datasets import SpectrogramDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from utils import get_bin_index,get_mutual_information\n",
    "from Metrics.mig import mig\n",
    "from Metrics.jemmig import jemmig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = Path(\"/net/projects/scratch/summer/valid_until_31_January_2024/ybrima/data/learning/SyncSpeech/dataset_16k.npz\")\n",
    "# Load data\n",
    "data = np.load(file_path, allow_pickle=True)\n",
    "x = data['x']\n",
    "y = data['y']\n",
    "metadata = data['metadata'].tolist()\n",
    "CLASSES = data['classes'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(metadata['freqs']),len(metadata['amps']),len(metadata['waveforms'])\n",
    "\n",
    "frequences = metadata['freqs']\n",
    "amps = metadata['amps']\n",
    "waveforms = np.array([CLASSES.index(x) for x in metadata['waveforms']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.04971324802977817"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_latents  = 2\n",
    "v_factor =  np.random.normal(0, 0.1, size=(10, n_latents))\n",
    "z_latents = np.random.normal(0, 1, size=(10, n_latents))\n",
    "\n",
    "\n",
    "\n",
    "mig(v_factor, z_latents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Split data\n",
    "x_train, x_val, y_train, y_val = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = SpectrogramDataset(x_train, y_train)\n",
    "val_dataset = SpectrogramDataset(x_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64)\n",
    "# Train 50 models\n",
    "num_models = 1\n",
    "\n",
    "latent_dim = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get input shape \n",
    "x_batch,x_spec_batch, y_batch = next(iter(train_loader))\n",
    "input_shape = x_spec_batch.shape\n",
    "\n",
    "\n",
    "model = VAEDeeper(latent_dim,input_shape)\n",
    "\n",
    "x_hat, z_mean, z_logvar = model(x_spec_batch)\n",
    "\n",
    "print(\"Shape of x_hat:\", x_hat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sr = 16000\n",
    "hop_length = 512\n",
    "# Display the Mel spectrogram\n",
    "idx =  np.random.randint(0, x_hat.shape[0])\n",
    "librosa.display.specshow(librosa.power_to_db(x_hat[idx].squeeze().detach().numpy(), ref=np.max),y_axis='mel', x_axis='time', sr = sr, hop_length=hop_length)\n",
    "plt.colorbar(format='%+2.0f dB')\n",
    "plt.title('Mel spectrogram')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "latent_dim = 16\n",
    "vae = VAEDeeper(latent_dim, input_shape).to(device)\n",
    "state_dict = torch.load('./Exports/vae2deeper.pth')\n",
    "\n",
    "# Load state dict\n",
    "vae.load_state_dict(state_dict)\n",
    "\n",
    "with torch.inference_mode():\n",
    "  x_hat, z_mean, z_logvar = vae(x_spec_batch.to(device))\n",
    "\n",
    "\n",
    "idx =  np.random.randint(0, x_hat.shape[0])\n",
    "\n",
    "fig, axs = plt.subplots(2, 3, figsize=(12,6), constrained_layout=True)\n",
    "\n",
    "# Extract mel specs \n",
    "mel_spec = x_spec_batch[idx].squeeze().detach().cpu().numpy()\n",
    "mel_spec_recon = x_hat[idx].squeeze().detach().cpu().numpy()\n",
    "\n",
    "# Waveforms\n",
    "waveform = librosa.feature.inverse.mel_to_audio(mel_spec, sr=sr, n_fft=2048, hop_length=512, n_iter=512)\n",
    "\n",
    "librosa.display.waveshow(waveform[:500], sr=sr, color='b', ax=axs[0,0], alpha=0.5)\n",
    "axs[0,0].set_xlabel('Time(s)')\n",
    "axs[0,0].set_ylabel('Amplitude')\n",
    "axs[0,0].set_title('Time domain', fontsize=12)\n",
    "\n",
    "waveform_recon = librosa.feature.inverse.mel_to_audio(mel_spec_recon, sr=sr, n_fft=2048, hop_length=512, n_iter=512)  \n",
    "# axs[1,0].waveshow(waveform_recon, sr=sr, color='b')\n",
    "librosa.display.waveshow(waveform_recon[:500], sr=sr, color='b', ax=axs[1,0], alpha=0.5)\n",
    "axs[1,0].set_xlabel('Time(s)')\n",
    "axs[1,0].set_ylabel('Amplitude')\n",
    "\n",
    "# Fourier Transforms\n",
    "ft = np.fft.fft(waveform)\n",
    "axs[0,1].plot(np.abs(ft)[:len(ft)//2])\n",
    "axs[0,1].set_xlabel('Frequency')\n",
    "axs[0,1].set_ylabel('Magnitude')\n",
    "axs[0,1].set_title('Fourier Transform', fontsize=12)\n",
    "\n",
    "ft_recon = np.fft.fft(waveform_recon)\n",
    "axs[1,1].plot(np.abs(ft_recon)[:len(ft_recon)//2]) \n",
    "axs[1,1].set_xlabel('Frequency')\n",
    "axs[1,1].set_ylabel('Magnitude')\n",
    "\n",
    "# Mel spectrograms\n",
    "librosa.display.specshow(librosa.power_to_db(mel_spec, ref=np.max),\n",
    "                  y_axis='mel', x_axis='s', sr=sr, \n",
    "                  hop_length=hop_length,ax=axs[0,2])\n",
    "axs[0,2].set_title('Mel spectrogram', fontsize=12)\n",
    "\n",
    "librosa.display.specshow(librosa.power_to_db(mel_spec_recon, ref=np.max),\n",
    "                  y_axis='mel', x_axis='s', sr=sr,\n",
    "                  hop_length=hop_length, ax=axs[1,2])\n",
    "# axs[1,2].set_title('Mel spectrogram')\n",
    "                   \n",
    "fig.suptitle(f'Original (top) vs. Reconstructed (bottom) of a {CLASSES[y_batch[idx]].capitalize()}', fontsize=16)\n",
    "# save the figure\n",
    "# plt.savefig('./Figures/Original_vs_Reconstructed.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mel_spec = x_hat[idx].squeeze().detach().cpu().numpy()\n",
    "waveform = librosa.feature.inverse.mel_to_audio(mel_spec, sr=sr, n_fft=2048, hop_length=512, n_iter=512)\n",
    "Audio(waveform, rate=sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Audio(x_batch[idx].squeeze().detach().cpu().numpy(), rate=sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming vae is your trained VAE model and latent_dim is the dimension of the latent space\n",
    "# Generate random samples from the latent space\n",
    "num_samples = 100  # Number of sounds to generate\n",
    "latent_samples = torch.randn(num_samples, latent_dim).to(device)  # Generate random samples\n",
    "\n",
    "# Decode the latent samples to generate new sounds\n",
    "with torch.no_grad():\n",
    "    vae.eval()\n",
    "    generated_mel_spectrograms = vae.decode(latent_samples)  # Decode the latent samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idxs =  np.random.randint(len(generated_mel_spectrograms))\n",
    "mel_spec = generated_mel_spectrograms[idxs].squeeze().detach().cpu().numpy()\n",
    "waveform = librosa.feature.inverse.mel_to_audio(mel_spec, sr=sr, n_fft=2048, hop_length=512, n_iter=512)\n",
    "\n",
    "Audio(waveform, rate=sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 8\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "vae = VAEDeep(latent_dim, input_shape).to(device)\n",
    "state_dict = torch.load(f'./Exports/vae2deep_{latent_dim}.pth')\n",
    "\n",
    "# Load state dict\n",
    "vae.load_state_dict(state_dict)\n",
    "\n",
    "with torch.inference_mode():\n",
    "  x_hat, z_mean, z_logvar = vae(x_spec_batch.to(device))\n",
    "\n",
    "\n",
    "idx =  np.random.randint(0, x_hat.shape[0])\n",
    "fig, axs = plt.subplots(2, 3, figsize=(12,6), constrained_layout=True)\n",
    "\n",
    "# Extract mel specs \n",
    "mel_spec = x_spec_batch[idx].squeeze().detach().cpu().numpy()\n",
    "mel_spec_recon = x_hat[idx].squeeze().detach().cpu().numpy()\n",
    "\n",
    "# Waveforms\n",
    "waveform = librosa.feature.inverse.mel_to_audio(mel_spec, sr=sr, n_fft=2048, hop_length=512, n_iter=512)\n",
    "\n",
    "librosa.display.waveshow(waveform[:100], sr=sr, color='b', ax=axs[0,0], alpha=0.5)\n",
    "axs[0,0].set_xlabel('Time(s)')\n",
    "axs[0,0].set_ylabel('Amplitude')\n",
    "axs[0,0].set_title('Time domain', fontsize=12)\n",
    "\n",
    "waveform_recon = librosa.feature.inverse.mel_to_audio(mel_spec_recon, sr=sr, n_fft=2048, hop_length=512, n_iter=512)  \n",
    "# axs[1,0].waveshow(waveform_recon, sr=sr, color='b')\n",
    "librosa.display.waveshow(waveform_recon[:100], sr=sr, color='b', ax=axs[1,0], alpha=0.5)\n",
    "axs[1,0].set_xlabel('Time(s)')\n",
    "axs[1,0].set_ylabel('Amplitude')\n",
    "\n",
    "# Fourier Transforms\n",
    "ft = np.fft.fft(waveform)\n",
    "axs[0,1].plot(np.abs(ft)[:len(ft)//2])\n",
    "axs[0,1].set_xlabel('Frequency')\n",
    "axs[0,1].set_ylabel('Magnitude')\n",
    "axs[0,1].set_title('Fourier Transform', fontsize=12)\n",
    "\n",
    "ft_recon = np.fft.fft(waveform_recon)\n",
    "axs[1,1].plot(np.abs(ft_recon)[:len(ft_recon)//2]) \n",
    "axs[1,1].set_xlabel('Frequency')\n",
    "axs[1,1].set_ylabel('Magnitude')\n",
    "\n",
    "# Mel spectrograms\n",
    "librosa.display.specshow(librosa.power_to_db(mel_spec, ref=np.max),\n",
    "                  y_axis='mel', x_axis='s', sr=sr, \n",
    "                  hop_length=hop_length,ax=axs[0,2])\n",
    "axs[0,2].set_title('Mel spectrogram', fontsize=12)\n",
    "\n",
    "librosa.display.specshow(librosa.power_to_db(mel_spec_recon, ref=np.max),\n",
    "                  y_axis='mel', x_axis='s', sr=sr,\n",
    "                  hop_length=hop_length, ax=axs[1,2])\n",
    "# axs[1,2].set_title('Mel spectrogram')\n",
    "                   \n",
    "fig.suptitle(f'Original (top) vs. Reconstructed (bottom) of a {CLASSES[y_batch[idx]].capitalize()}', fontsize=16)\n",
    "# save the figure\n",
    "# plt.savefig('./Figures/Original_vs_Reconstructed.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming vae is your trained VAE model and latent_dim is the dimension of the latent space\n",
    "# Generate random samples from the latent space\n",
    "num_samples = 100  # Number of sounds to generate\n",
    "latent_samples = torch.randn(num_samples, latent_dim).to(device)  # Generate random samples\n",
    "\n",
    "# Decode the latent samples to generate new sounds\n",
    "with torch.no_grad():\n",
    "    vae.eval()\n",
    "    generated_mel_spectrograms = vae.decode(latent_samples)  # Decode the latent samples\n",
    "\n",
    "# Random generated mel spectrogram\n",
    "# Create figure and axes\n",
    "fig, axs = plt.subplots(1, 3, figsize=(16, 4))\n",
    "\n",
    "idx =  np.random.randint(len(generated_mel_spectrograms))\n",
    "mel_spec = generated_mel_spectrograms[idx].squeeze().detach().cpu().numpy()\n",
    "waveform = librosa.feature.inverse.mel_to_audio(mel_spec, sr=sr, n_fft=2048, hop_length=512, n_iter=512)\n",
    "\n",
    "# Waveform\n",
    "librosa.display.waveshow(waveform[:100], sr=sr, ax=axs[0], alpha=0.5, color='b')\n",
    "axs[0].set_title('Waveform')\n",
    "axs[0].set_xlabel('Time(s)')\n",
    "axs[0].set_ylabel('Amplitude')\n",
    "\n",
    "# Fourier Transform \n",
    "ft = np.fft.fft(waveform)\n",
    "axs[1].plot(np.abs(ft)[:len(ft)//2])\n",
    "axs[1].set_title('Fourier Transform')\n",
    "axs[1].set_xlabel('Frequency')\n",
    "axs[1].set_ylabel('Magnitude')\n",
    "\n",
    "# Mel Spectrogram\n",
    "librosa.display.specshow(librosa.power_to_db(mel_spec, ref=np.max), \n",
    "                y_axis='mel', x_axis='s', sr=sr, \n",
    "                hop_length=hop_length, ax=axs[2])\n",
    "axs[2].set_title('Mel Spectrogram')  \n",
    "# axs[2].set_ylabel('Magnitude')\n",
    "fig.suptitle('Generated Sample', fontsize=16)  \n",
    "plt.tight_layout()\n",
    "# save the figure\n",
    "# plt.savefig('./Figures/Generated_sample.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 8\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "vae = VAEDeep(latent_dim, input_shape).to(device)\n",
    "state_dict = torch.load(f'./Exports/vae2deep_8_beta20.pth')\n",
    "\n",
    "# Load state dict\n",
    "vae.load_state_dict(state_dict)\n",
    "\n",
    "with torch.inference_mode():\n",
    "  x_hat, z_mean, z_logvar = vae(x_spec_batch.to(device))\n",
    "\n",
    "\n",
    "idx =  np.random.randint(0, x_hat.shape[0])\n",
    "fig, axs = plt.subplots(2, 3, figsize=(12,6), constrained_layout=True)\n",
    "\n",
    "# Extract mel specs \n",
    "mel_spec = x_spec_batch[idx].squeeze().detach().cpu().numpy()\n",
    "mel_spec_recon = x_hat[idx].squeeze().detach().cpu().numpy()\n",
    "\n",
    "# Waveforms\n",
    "waveform = librosa.feature.inverse.mel_to_audio(mel_spec, sr=sr, n_fft=2048, hop_length=512, n_iter=512)\n",
    "\n",
    "librosa.display.waveshow(waveform[:100], sr=sr, color='b', ax=axs[0,0], alpha=0.5)\n",
    "axs[0,0].set_xlabel('Time(s)')\n",
    "axs[0,0].set_ylabel('Amplitude')\n",
    "axs[0,0].set_title('Time domain', fontsize=12)\n",
    "\n",
    "waveform_recon = librosa.feature.inverse.mel_to_audio(mel_spec_recon, sr=sr, n_fft=2048, hop_length=512, n_iter=512)  \n",
    "# axs[1,0].waveshow(waveform_recon, sr=sr, color='b')\n",
    "librosa.display.waveshow(waveform_recon[:100], sr=sr, color='b', ax=axs[1,0], alpha=0.5)\n",
    "axs[1,0].set_xlabel('Time(s)')\n",
    "axs[1,0].set_ylabel('Amplitude')\n",
    "\n",
    "# Fourier Transforms\n",
    "ft = np.fft.fft(waveform)\n",
    "axs[0,1].plot(np.abs(ft)[:len(ft)//2])\n",
    "axs[0,1].set_xlabel('Frequency')\n",
    "axs[0,1].set_ylabel('Magnitude')\n",
    "axs[0,1].set_title('Fourier Transform', fontsize=12)\n",
    "\n",
    "ft_recon = np.fft.fft(waveform_recon)\n",
    "axs[1,1].plot(np.abs(ft_recon)[:len(ft_recon)//2]) \n",
    "axs[1,1].set_xlabel('Frequency')\n",
    "axs[1,1].set_ylabel('Magnitude')\n",
    "\n",
    "# Mel spectrograms\n",
    "librosa.display.specshow(librosa.power_to_db(mel_spec, ref=np.max),\n",
    "                  y_axis='mel', x_axis='s', sr=sr, \n",
    "                  hop_length=hop_length,ax=axs[0,2])\n",
    "axs[0,2].set_title('Mel spectrogram', fontsize=12)\n",
    "\n",
    "librosa.display.specshow(librosa.power_to_db(mel_spec_recon, ref=np.max),\n",
    "                  y_axis='mel', x_axis='s', sr=sr,\n",
    "                  hop_length=hop_length, ax=axs[1,2])\n",
    "# axs[1,2].set_title('Mel spectrogram')\n",
    "                   \n",
    "fig.suptitle(f'Original (top) vs. Reconstructed (bottom) of a {CLASSES[y_batch[idx]].capitalize()}', fontsize=16)\n",
    "# save the figure\n",
    "# plt.savefig('./Figures/Original_vs_Reconstructed.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mel_spec = x_hat[idx].squeeze().detach().cpu().numpy()\n",
    "waveform = librosa.feature.inverse.mel_to_audio(mel_spec, sr=sr, n_fft=2048, hop_length=512, n_iter=512)\n",
    "Audio(waveform, rate=sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mel_spec = x_spec_batch[idx].squeeze().detach().cpu().numpy()\n",
    "waveform = librosa.feature.inverse.mel_to_audio(mel_spec, sr=sr, n_fft=2048, hop_length=512, n_iter=512)\n",
    "Audio(waveform, rate=sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Audio(x_batch[idx].squeeze().detach().cpu().numpy(), rate=sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming vae is your trained VAE model and latent_dim is the dimension of the latent space\n",
    "# Generate random samples from the latent space\n",
    "num_samples = 100  # Number of sounds to generate\n",
    "latent_samples = torch.randn(num_samples, latent_dim).to(device)  # Generate random samples\n",
    "\n",
    "# Decode the latent samples to generate new sounds\n",
    "with torch.no_grad():\n",
    "    vae.eval()\n",
    "    generated_mel_spectrograms = vae.decode(latent_samples)  # Decode the latent samples\n",
    "\n",
    "# Random generated mel spectrogram\n",
    "# Create figure and axes\n",
    "fig, axs = plt.subplots(1, 3, figsize=(16, 4))\n",
    "\n",
    "idx =  np.random.randint(len(generated_mel_spectrograms))\n",
    "mel_spec = generated_mel_spectrograms[idx].squeeze().detach().cpu().numpy()\n",
    "waveform = librosa.feature.inverse.mel_to_audio(mel_spec, sr=sr, n_fft=2048, hop_length=512, n_iter=512)\n",
    "\n",
    "# Waveform\n",
    "librosa.display.waveshow(waveform[:100], sr=sr, ax=axs[0], alpha=0.5, color='b')\n",
    "axs[0].set_title('Waveform')\n",
    "axs[0].set_xlabel('Time(s)')\n",
    "axs[0].set_ylabel('Amplitude')\n",
    "\n",
    "# Fourier Transform \n",
    "ft = np.fft.fft(waveform)\n",
    "axs[1].plot(np.abs(ft)[:len(ft)//2])\n",
    "axs[1].set_title('Fourier Transform')\n",
    "axs[1].set_xlabel('Frequency')\n",
    "axs[1].set_ylabel('Magnitude')\n",
    "\n",
    "# Mel Spectrogram\n",
    "librosa.display.specshow(librosa.power_to_db(mel_spec, ref=np.max), \n",
    "                y_axis='mel', x_axis='s', sr=sr, \n",
    "                hop_length=hop_length, ax=axs[2])\n",
    "axs[2].set_title('Mel Spectrogram')  \n",
    "# axs[2].set_ylabel('Magnitude')\n",
    "fig.suptitle('Generated Sample', fontsize=16)  \n",
    "plt.tight_layout()\n",
    "# save the figure\n",
    "# plt.savefig('./Figures/Generated_sample.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming vae is your trained VAE model and latent_dim is the dimension of the latent space\n",
    "# Generate random samples from the latent space\n",
    "num_samples = 100  # Number of sounds to generate\n",
    "latent_samples = torch.randn(num_samples, latent_dim).to(device)  # Generate random samples\n",
    "\n",
    "# Decode the latent samples to generate new sounds\n",
    "with torch.no_grad():\n",
    "    vae.eval()\n",
    "    generated_mel_spectrograms = vae.decode(latent_samples)  # Decode the latent samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx =  np.random.randint(len(generated_mel_spectrograms))\n",
    "mel_spec = generated_mel_spectrograms[idx].squeeze().detach().cpu().numpy()\n",
    "waveform = librosa.feature.inverse.mel_to_audio(mel_spec, sr=sr, n_fft=2048, hop_length=512, n_iter=512)\n",
    "\n",
    "Audio(waveform, rate=sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 32\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "vae = VAEDeepStable(latent_dim, input_shape).to(device)\n",
    "state_dict = torch.load('./Exports/vae2deepstable_32.pth')\n",
    "\n",
    "# Load state dict\n",
    "vae.load_state_dict(state_dict)\n",
    "\n",
    "with torch.inference_mode():\n",
    "  x_hat, z_mean, z_logvar = vae(x_spec_batch.to(device))\n",
    "\n",
    "\n",
    "idx =  np.random.randint(0, x_hat.shape[0])\n",
    "\n",
    "fig, axs = plt.subplots(2, 3, figsize=(12,6), constrained_layout=True)\n",
    "\n",
    "# Extract mel specs \n",
    "mel_spec = x_spec_batch[idx].squeeze().detach().cpu().numpy()\n",
    "mel_spec_recon = x_hat[idx].squeeze().detach().cpu().numpy()\n",
    "\n",
    "# Waveforms\n",
    "waveform = librosa.feature.inverse.mel_to_audio(mel_spec, sr=sr, n_fft=2048, hop_length=512, n_iter=512)\n",
    "\n",
    "librosa.display.waveshow(waveform, sr=sr, color='b', ax=axs[0,0], alpha=0.5)\n",
    "axs[0,0].set_xlabel('Time(s)')\n",
    "axs[0,0].set_ylabel('Amplitude')\n",
    "axs[0,0].set_title('Time domain', fontsize=12)\n",
    "\n",
    "waveform_recon = librosa.feature.inverse.mel_to_audio(mel_spec_recon, sr=sr, n_fft=2048, hop_length=512, n_iter=512)  \n",
    "# axs[1,0].waveshow(waveform_recon, sr=sr, color='b')\n",
    "librosa.display.waveshow(waveform_recon, sr=sr, color='b', ax=axs[1,0], alpha=0.5)\n",
    "axs[1,0].set_xlabel('Time(s)')\n",
    "axs[1,0].set_ylabel('Amplitude')\n",
    "\n",
    "# Fourier Transforms\n",
    "ft = np.fft.fft(waveform)\n",
    "axs[0,1].plot(np.abs(ft)[:len(ft)//2])\n",
    "axs[0,1].set_xlabel('Frequency')\n",
    "axs[0,1].set_ylabel('Magnitude')\n",
    "axs[0,1].set_title('Fourier Transform', fontsize=12)\n",
    "\n",
    "ft_recon = np.fft.fft(waveform_recon)\n",
    "axs[1,1].plot(np.abs(ft_recon)[:len(ft_recon)//2]) \n",
    "axs[1,1].set_xlabel('Frequency')\n",
    "axs[1,1].set_ylabel('Magnitude')\n",
    "\n",
    "# Mel spectrograms\n",
    "librosa.display.specshow(librosa.power_to_db(mel_spec, ref=np.max),\n",
    "                  y_axis='mel', x_axis='s', sr=sr, \n",
    "                  hop_length=hop_length,ax=axs[0,2])\n",
    "axs[0,2].set_title('Mel spectrogram', fontsize=12)\n",
    "\n",
    "librosa.display.specshow(librosa.power_to_db(mel_spec_recon, ref=np.max),\n",
    "                  y_axis='mel', x_axis='s', sr=sr,\n",
    "                  hop_length=hop_length, ax=axs[1,2])\n",
    "# axs[1,2].set_title('Mel spectrogram')\n",
    "                   \n",
    "fig.suptitle(f'Original (top) vs. Reconstructed (bottom) of a {CLASSES[y_batch[idx]].capitalize()}', fontsize=16)\n",
    "# save the figure\n",
    "# plt.savefig('./Figures/Original_vs_Reconstructed.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mel_spec = x_hat[idx].squeeze().detach().cpu().numpy()\n",
    "waveform = librosa.feature.inverse.mel_to_audio(mel_spec, sr=sr, n_fft=2048, hop_length=512, n_iter=512)\n",
    "Audio(waveform, rate=sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mel_spec = x_spec_batch[idx].squeeze().detach().cpu().numpy()\n",
    "waveform = librosa.feature.inverse.mel_to_audio(mel_spec, sr=sr, n_fft=2048, hop_length=512, n_iter=512)\n",
    "Audio(waveform, rate=sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming vae is your trained VAE model and latent_dim is the dimension of the latent space\n",
    "# Generate random samples from the latent space\n",
    "num_samples = 100  # Number of sounds to generate\n",
    "latent_samples = torch.randn(num_samples, latent_dim).to(device)  # Generate random samples\n",
    "\n",
    "# Decode the latent samples to generate new sounds\n",
    "with torch.no_grad():\n",
    "    vae.eval()\n",
    "    generated_mel_spectrograms = vae.decode(latent_samples)  # Decode the latent samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx =  np.random.randint(len(generated_mel_spectrograms))\n",
    "mel_spec = generated_mel_spectrograms[idx].squeeze().detach().cpu().numpy()\n",
    "waveform = librosa.feature.inverse.mel_to_audio(mel_spec, sr=sr, n_fft=2048, hop_length=512, n_iter=512)\n",
    "\n",
    "Audio(waveform, rate=sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming vae is your trained VAE model and latent_dim is the dimension of the latent space\n",
    "# Generate random samples from the latent space\n",
    "num_samples = 100  # Number of sounds to generate\n",
    "latent_samples = torch.randn(num_samples, latent_dim).to(device)  # Generate random samples\n",
    "\n",
    "# Decode the latent samples to generate new sounds\n",
    "with torch.no_grad():\n",
    "    vae.eval()\n",
    "    generated_mel_spectrograms = vae.decode(latent_samples)  # Decode the latent samples\n",
    "\n",
    "# Random generated mel spectrogram\n",
    "# Create figure and axes\n",
    "fig, axs = plt.subplots(1, 3, figsize=(16, 4))\n",
    "\n",
    "idx =  np.random.randint(len(generated_mel_spectrograms))\n",
    "mel_spec = generated_mel_spectrograms[idx].squeeze().detach().cpu().numpy()\n",
    "waveform = librosa.feature.inverse.mel_to_audio(mel_spec, sr=sr, n_fft=2048, hop_length=512, n_iter=512)\n",
    "\n",
    "# Waveform\n",
    "librosa.display.waveshow(waveform, sr=sr, ax=axs[0], alpha=0.5, color='b')\n",
    "axs[0].set_title('Waveform')\n",
    "axs[0].set_xlabel('Time(s)')\n",
    "axs[0].set_ylabel('Amplitude')\n",
    "\n",
    "# Fourier Transform \n",
    "ft = np.fft.fft(waveform)\n",
    "axs[1].plot(np.abs(ft)[:len(ft)//2])\n",
    "axs[1].set_title('Fourier Transform')\n",
    "axs[1].set_xlabel('Frequency')\n",
    "axs[1].set_ylabel('Magnitude')\n",
    "\n",
    "# Mel Spectrogram\n",
    "librosa.display.specshow(librosa.power_to_db(mel_spec, ref=np.max), \n",
    "                y_axis='mel', x_axis='s', sr=sr, \n",
    "                hop_length=hop_length, ax=axs[2])\n",
    "axs[2].set_title('Mel Spectrogram')  \n",
    "# axs[2].set_ylabel('Magnitude')\n",
    "fig.suptitle('Generated Sample', fontsize=16)  \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_batch,spec_batch, y_batch = next(iter(train_loader))\n",
    "n = 2\n",
    "idx = np.random.randint(len(x_batch), size=n)\n",
    "# Plot\n",
    "fig, axs = plt.subplots(n, 2, figsize=(12,6))\n",
    "# fig.patch.set_facecolor('none') \n",
    "for i in range(n):\n",
    "  # Waveform \n",
    "  librosa.display.waveshow(x_batch[idx[i]].squeeze().detach().cpu().numpy()[:1000], sr=sr, color='b', ax=axs[i, 0])\n",
    "  # axs[i, 0].plot(x_batch[i].squeeze().detach().cpu().numpy())\n",
    "  axs[i, 0].set_title(f'{CLASSES[y_batch[idx[i]]].capitalize() }')\n",
    "  axs[i, 0].set_xlabel('Time')\n",
    "  axs[i, 0].set_ylabel('Amplitude')\n",
    "\n",
    "  # Spectrogram\n",
    "  # axs[i, 1].imshow(librosa.power_to_db(spec_batch[idx[i]].squeeze().detach().cpu().numpy()), origin=\"lower\", aspect=\"auto\", interpolation=\"nearest\")\n",
    "  librosa.display.specshow(librosa.amplitude_to_db(spec_batch[idx[i]].squeeze().detach().cpu().numpy()),win_length=512, sr=sr, x_axis='time', y_axis='log', ax=axs[i, 1])\n",
    "  axs[i, 1].set_title(f'{CLASSES[y_batch[idx[i]]].capitalize() }')\n",
    "  axs[i, 1].set_xlabel('Time')\n",
    "  axs[i, 1].set_ylabel('Frequency')\n",
    "plt.tight_layout()\n",
    "# save the figure\n",
    "plt.savefig('./Figures/Samples2.png', dpi=300, transparent=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "disen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
